{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4616b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import srsly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e610c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where our entity mapping file is\n",
    "entity_map = srsly.read_json(\"../assets/mapped_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f755bc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['Person-Individual',\n",
       "  'Person-Collective',\n",
       "  'Organization-Political',\n",
       "  'Organization-Government',\n",
       "  'Organization-Military',\n",
       "  'Organization-Other',\n",
       "  'Location',\n",
       "  'Object',\n",
       "  'Time',\n",
       "  'Event-Local',\n",
       "  'Event-International',\n",
       "  'Production-Media',\n",
       "  'Production-Government',\n",
       "  'Production-Doctrine',\n",
       "  'Numerical Statistics'],\n",
       " 'iob_mapping': {'0': 'O',\n",
       "  '1': 'B-Person-Individual',\n",
       "  '2': 'I-Person-Individual',\n",
       "  '3': 'B-Person-Collective',\n",
       "  '4': 'I-Person-Collective',\n",
       "  '5': 'B-Organization-Political',\n",
       "  '6': 'I-Organization-Political',\n",
       "  '7': 'B-Organization-Government',\n",
       "  '8': 'I-Organization-Government',\n",
       "  '9': 'B-Organization-Military',\n",
       "  '10': 'I-Organization-Military',\n",
       "  '11': 'B-Organization-Other',\n",
       "  '12': 'I-Organization-Other',\n",
       "  '13': 'B-Location',\n",
       "  '14': 'I-Location',\n",
       "  '15': 'B-Object',\n",
       "  '16': 'I-Object',\n",
       "  '17': 'B-Time',\n",
       "  '18': 'I-Time',\n",
       "  '19': 'B-Event-Local',\n",
       "  '20': 'I-Event-Local',\n",
       "  '21': 'B-Event-International',\n",
       "  '22': 'I-Event-International',\n",
       "  '23': 'B-Production-Media',\n",
       "  '24': 'I-Production-Media',\n",
       "  '25': 'B-Production-Government',\n",
       "  '26': 'I-Production-Government',\n",
       "  '27': 'B-Production-Doctrine',\n",
       "  '28': 'I-Production-Doctrine',\n",
       "  '29': 'B-Numerical Statistics',\n",
       "  '30': 'I-Numerical Statistics'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all the entities\n",
    "entities = entity_map[\"labels\"]\n",
    "entity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4beef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entity_to_iob(entity_type : str, inside=False) -> int :\n",
    "    iob_num = 0\n",
    "\n",
    "    # might slow things but adding this for checking\n",
    "    if entity_type in entities:\n",
    "        iob_num = entities.index(entity_type)\n",
    "    else:\n",
    "        print(\"Error entity type not found\")\n",
    "        return 0;\n",
    "\n",
    "    # note: the zeroth index in the array is always mapped to the first index in the map\n",
    "    iob_num = iob_num * 2 + 1\n",
    "\n",
    "    if inside:\n",
    "        return iob_num + 1\n",
    "    else:\n",
    "        return iob_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbee188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from typing import Literal\n",
    "\n",
    "def spacy_to_hf(raw_spacy_directory : str, set:Literal[\"train\", \"dev\", \"test\"]):\n",
    "    doc_bin = DocBin().from_disk(f\"{raw_spacy_directory}/{set}.spacy\")\n",
    "    \n",
    "    # The model used to construct the doc object\n",
    "    nlp = spacy.blank(\"tl\")  # or the language your corpus uses\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    entities = []\n",
    "    iob_tags = []\n",
    "    ids = []\n",
    "\n",
    "    row_indices = []\n",
    "    publications = []\n",
    "    years = []\n",
    "\n",
    "    id_iterator = 0\n",
    "\n",
    "    for doc in docs:\n",
    "        # the tokens\n",
    "        texts.append(doc.text)\n",
    "        token_texts = [t.text for t in doc]\n",
    "        \n",
    "        # some metadatas\n",
    "        publication = doc.user_data[\"Publication\"]\n",
    "        row_index = int(doc.user_data[\"Row_Index\"])\n",
    "        year = doc.user_data[\"Year\"]\n",
    "\n",
    "        # We'll follow the I-O-B scheme\n",
    "        token_labels = [\"O\"] * len(doc)\n",
    "        token_tags = [0] * len(doc)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            token_tags[ent.start] = convert_entity_to_iob(ent.label_);\n",
    "            token_labels[ent.start] = \"B-\" + ent.label_\n",
    "            for i in range(ent.start + 1, ent.end):\n",
    "                token_labels[i] = \"I-\" + ent.label_\n",
    "                token_tags[i] = convert_entity_to_iob(ent.label_, inside=True);\n",
    "        \n",
    "        tokens.append(token_texts)\n",
    "        entities.append(token_labels)\n",
    "        iob_tags.append(token_tags)\n",
    "        ids.append(id_iterator)\n",
    "        \n",
    "        publications.append(publication)\n",
    "        row_indices.append(row_index)\n",
    "        years.append(year)\n",
    "\n",
    "\n",
    "        id_iterator += 1\n",
    "        # end of for loop\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"id\": ids,\n",
    "        \"tokens\": tokens,\n",
    "        \"entities\": entities, \n",
    "        \"ner_tags\": iob_tags,\n",
    "        \"Row_Index\": row_indices,\n",
    "        \"Year\": years,\n",
    "        \"Publication\": publications\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9751bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 1891\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 271\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 538\n",
      "    })\n",
      "})\n",
      "Sample {'id': 0, 'tokens': ['Hiniling', 'noong', 'nakaraang', 'linggo', 'ng', 'United', 'Nationalist', 'Democratic', 'Organization', '(', 'UNIDO', ')', 'na', 'pawalang', '-', 'bisa', 'ang', 'halalan', 'sa', '16', 'na', 'lalawigan', 'at', 'kagyat', 'na', 'maglunsad', 'ng', 'bagong', 'eleksiyon', 'sa', 'mga', 'nasabing', 'lugar', '.'], 'entities': ['O', 'O', 'O', 'O', 'O', 'B-Organization-Political', 'I-Organization-Political', 'I-Organization-Political', 'I-Organization-Political', 'O', 'B-Organization-Political', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Event-Local', 'O', 'B-Numerical Statistics', 'I-Numerical Statistics', 'I-Numerical Statistics', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner_tags': [0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 5, 0, 0, 0, 0, 0, 0, 19, 0, 29, 30, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Row_Index': 2228, 'Year': '1984', 'Publication': 'Ang Tinig ng Masa'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ba5dcdf9bc44d3afeb95ef5e666247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eede14d6a3934158b8d52fb0daac36b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8e91d84c4441f5bb6b1c68efe06fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/538 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load all three sets\n",
    "\n",
    "corpus_directory = \"../assets/corpus/\"\n",
    "dataset_name = \"batch_1-2\"\n",
    "training_size = 2162\n",
    "\n",
    "test_dataset  = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy\", \"test\")\n",
    "train_dataset = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy/subset/{training_size}\", \"train\")\n",
    "dev_dataset   = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy/subset/{training_size}\", \"dev\")\n",
    "\n",
    "\n",
    "# 2. Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": dev_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)\n",
    "print(\"Sample\",dataset_dict[\"train\"][0])  # first training sample\n",
    "\n",
    "# 3. Save locally\n",
    "dataset_dict.save_to_disk(f\"{corpus_directory}/{dataset_name}/dataset_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0663ea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 1891\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 271\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 538\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_disk(f\"{corpus_directory}/{dataset_name}/dataset_full\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab47c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8babe5efe3429ba123f068668d02dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a79d7e5c9db44e1a236617d67e11b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73072f3568c4f9c811125f45437baf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700277bae7e84b418fd2537ef6fffdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d102384f02684e9dbbb9c4c106cc1380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7665187a5747d68ee662a73937f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4827022ac25241919cd5780672bed879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1db62ce42042e9a5bf9015e4ba74c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e016afdae1648dcb9c175ff40fde5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a805b624f8be4f89a96bb64a58ce3b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a732e877c443209694e503ec5cfd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9cea84596849549f071aee9e480888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os;\n",
    "\n",
    "push_to_hub = True\n",
    "\n",
    "owner = \"etdvprg\"\n",
    "hf_dataset_name = \"PHMartialLaw-NER_b12\"\n",
    "\n",
    "if push_to_hub:\n",
    "    api_token = os.getenv(\"HF_TOKEN\")\n",
    "    ds.push_to_hub(f\"{owner}/{hf_dataset_name}\", token=api_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
