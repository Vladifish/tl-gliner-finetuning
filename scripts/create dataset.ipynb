{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4616b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import srsly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e610c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where our entity mapping file is\n",
    "entity_map = srsly.read_json(\"../assets/mapped_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f755bc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['Person-Individual',\n",
       "  'Person-Collective',\n",
       "  'Organization-Political',\n",
       "  'Organization-Government',\n",
       "  'Organization-Military',\n",
       "  'Organization-Other',\n",
       "  'Location',\n",
       "  'Object',\n",
       "  'Time',\n",
       "  'Event-Local',\n",
       "  'Event-International',\n",
       "  'Production-Media',\n",
       "  'Production-Government',\n",
       "  'Production-Doctrine',\n",
       "  'Numerical Statistics'],\n",
       " 'iob_mapping': {'0': 'O',\n",
       "  '1': 'B-Person-Individual',\n",
       "  '2': 'I-Person-Individual',\n",
       "  '3': 'B-Person-Collective',\n",
       "  '4': 'I-Person-Collective',\n",
       "  '5': 'B-Organization-Political',\n",
       "  '6': 'I-Organization-Political',\n",
       "  '7': 'B-Organization-Government',\n",
       "  '8': 'I-Organization-Government',\n",
       "  '9': 'B-Organization-Military',\n",
       "  '10': 'I-Organization-Military',\n",
       "  '11': 'B-Organization-Other',\n",
       "  '12': 'I-Organization-Other',\n",
       "  '13': 'B-Location',\n",
       "  '14': 'I-Location',\n",
       "  '15': 'B-Object',\n",
       "  '16': 'I-Object',\n",
       "  '17': 'B-Time',\n",
       "  '18': 'I-Time',\n",
       "  '19': 'B-Event-Local',\n",
       "  '20': 'I-Event-Local',\n",
       "  '21': 'B-Event-International',\n",
       "  '22': 'I-Event-International',\n",
       "  '23': 'B-Production-Media',\n",
       "  '24': 'I-Production-Media',\n",
       "  '25': 'B-Production-Government',\n",
       "  '26': 'I-Production-Government',\n",
       "  '27': 'B-Production-Doctrine',\n",
       "  '28': 'I-Production-Doctrine',\n",
       "  '29': 'B-Numerical Statistics',\n",
       "  '30': 'I-Numerical Statistics'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all the entities\n",
    "entities = entity_map[\"labels\"]\n",
    "entity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4beef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entity_to_iob(entity_type : str, inside=False) -> int :\n",
    "    iob_num = 0\n",
    "\n",
    "    # might slow things but adding this for checking\n",
    "    if entity_type in entities:\n",
    "        iob_num = entities.index(entity_type)\n",
    "    else:\n",
    "        print(\"Error entity type not found\")\n",
    "        return 0;\n",
    "\n",
    "    # note: the zeroth index in the array is always mapped to the first index in the map\n",
    "    iob_num = iob_num * 2 + 1\n",
    "\n",
    "    if inside:\n",
    "        return iob_num + 1\n",
    "    else:\n",
    "        return iob_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcbee188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from typing import Literal\n",
    "\n",
    "def spacy_to_hf(raw_spacy_directory : str, set:Literal[\"train\", \"dev\", \"test\"]):\n",
    "    doc_bin = DocBin().from_disk(f\"{raw_spacy_directory}/{set}.spacy\")\n",
    "    \n",
    "    # The model used to construct the doc object\n",
    "    nlp = spacy.blank(\"tl\")  # or the language your corpus uses\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    entities = []\n",
    "    iob_tags = []\n",
    "    ids = []\n",
    "\n",
    "    row_indices = []\n",
    "    publications = []\n",
    "    years = []\n",
    "\n",
    "    id_iterator = 0\n",
    "\n",
    "    for doc in docs:\n",
    "        # the tokens\n",
    "        texts.append(doc.text)\n",
    "        token_texts = [t.text for t in doc]\n",
    "        \n",
    "        # some metadatas\n",
    "        publication = doc.user_data[\"Publication\"]\n",
    "        row_index = int(doc.user_data[\"Row_Index\"])\n",
    "        year = doc.user_data[\"Year\"]\n",
    "\n",
    "        # We'll follow the I-O-B scheme\n",
    "        token_labels = [\"O\"] * len(doc)\n",
    "        token_tags = [0] * len(doc)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            token_tags[ent.start] = convert_entity_to_iob(ent.label_);\n",
    "            token_labels[ent.start] = \"B-\" + ent.label_\n",
    "            for i in range(ent.start + 1, ent.end):\n",
    "                token_labels[i] = \"I-\" + ent.label_\n",
    "                token_tags[i] = convert_entity_to_iob(ent.label_, inside=True);\n",
    "        \n",
    "        tokens.append(token_texts)\n",
    "        entities.append(token_labels)\n",
    "        iob_tags.append(token_tags)\n",
    "        ids.append(id_iterator)\n",
    "        \n",
    "        publications.append(publication)\n",
    "        row_indices.append(row_index)\n",
    "        years.append(year)\n",
    "\n",
    "\n",
    "        id_iterator += 1\n",
    "        # end of for loop\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"id\": ids,\n",
    "        \"tokens\": tokens,\n",
    "        \"entities\": entities, \n",
    "        \"ner_tags\": iob_tags,\n",
    "        \"Row_Index\": row_indices,\n",
    "        \"Year\": years,\n",
    "        \"Publication\": publications\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9751bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 1945\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 217\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
      "        num_rows: 538\n",
      "    })\n",
      "})\n",
      "Sample {'id': 0, 'tokens': ['Nauna', 'rito', ',', 'isang', 'yunit', 'ng', 'BHB', 'na', 'kumikilos', 'sa', 'gitnang', 'Isabela', 'ang', 'nakapatay', 'ng', 'apat', 'na', 'opisyal', 'at', 'tatlong', 'tauhan', 'ng', 'AFP', 'sa', 'isang', 'ambus', '.'], 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'B-Organization-Military', 'O', 'O', 'O', 'O', 'B-Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Organization-Military', 'O', 'O', 'O', 'O'], 'ner_tags': [0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0], 'Row_Index': 7160, 'Year': '1974', 'Publication': 'Taliba ng Bayan'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9b3655d3d04b6087a77424beab6fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd638dbbd234007a39199d6765feab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8126fef6944624a8a669cb83738272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/538 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load all three sets\n",
    "\n",
    "corpus_directory = \"../assets/corpus/\"\n",
    "dataset_name = \"batch_1-2\"\n",
    "training_size = 2162\n",
    "\n",
    "test_dataset  = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy\", \"test\")\n",
    "train_dataset = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy/subset/{training_size}\", \"train\")\n",
    "dev_dataset   = spacy_to_hf(f\"{corpus_directory}/{dataset_name}/spacy/subset/{training_size}\", \"dev\")\n",
    "\n",
    "\n",
    "# 2. Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": dev_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)\n",
    "print(\"Sample\",dataset_dict[\"train\"][0])  # first training sample\n",
    "\n",
    "# 3. Save locally\n",
    "dataset_dict.save_to_disk(f\"{corpus_directory}/{dataset_name}/dataset_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0663ea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 1945\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 217\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'entities', 'ner_tags', 'Row_Index', 'Year', 'Publication'],\n",
       "        num_rows: 538\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_disk(f\"{corpus_directory}/{dataset_name}/dataset_full\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bab47c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cfcfd827654bafae6262c885cf7289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45c705c4c4743269c0382ba66a69b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94fe31b23a34007a97920c4b9d01d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36191e37d7744315af2d57e770310883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fbe430bf88404d9a6d68a9059665fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d5d9c989924a6a8f7901a43e43b0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6f6cb5f26942c0b2086234b9eede46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d90d1dceff4367af60ccdb949eeeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035704d90abb4e609e2c25dded7b907b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614f5b7d6fbb473e98f966a6baba7009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c700f5b352164e0eb0a82fc099db0542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0875aa28994061b56982f4f6840e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b05b09e7424d4e97a46d39ae1a2b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/696 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os;\n",
    "\n",
    "push_to_hub = True\n",
    "\n",
    "owner = \"etdvprg\"\n",
    "hf_dataset_name = \"PHMartialLaw-NER_b12\"\n",
    "\n",
    "if push_to_hub:\n",
    "    api_token = os.getenv(\"HF_TOKEN\")\n",
    "    ds.push_to_hub(f\"{owner}/{hf_dataset_name}\", token=api_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
